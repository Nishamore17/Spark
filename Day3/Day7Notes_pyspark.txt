 SPARK
 SPARK IS A GENERAL PURPOSE PROCESSING/ECOSYSTEM[APIS] WHICH IS WRITTEN FUNCTIONAL BASED LANG CALLED "SCALA".

SCALA --> SUPERSET OF JAVA; RUNS IN JVM

FEATURES
1. SPARK IS A DISTRIBUTED MEMORY PROCESSING
2. SPARK CAN READ OR WRITE THE DATE FROM/TO SEVERAL SOURCES
   --> LOCAL FILESYSTEM
   --> HDFS
   --> S3 BUCKET [AWS]
   --> ADLS [AZURE]
   --> NOSQL [MONGODB/CASSANDRA]
   --> KAFKA
   --> RDBMS
   --> HIVE METASTORE
3. SPARK IS LANGUAGE INDEPENDENT
   --> CORE SPARK: scala
   --> PYTHON: pyspark
   --> JAVA: spark java
   --> R: sparklyr
   --> SPARK SQL: sql [default]
   --> SPARK ML: machine learning
   --> STREAMING: spark streams

4. SPARK IS PLATFORM INDEPENDENT

5. SPARK HAS DISTRIBUTORS: CLOUDERA, DATABRICKS, AZURE,AWS

6. SPARK CAN ALSO INTEGRATE WITH VISUALIZATION


-----
1. SPARK RUNS WITH THE HELP OF MAIN CLASS PROGRAM CALLED "driver"
2. DRIVER IS LAUNCHED BASED ON THE MODE WE ARE STARTING THE SPARK
   MODE IS DEFINED USING "master" --> spark master

   spark-shell --master=local/standalone/yarn/kubernetes
   NOTE: we cn set the default mode in spark-defaults.conf file.

3. SPARK CAN RUN IN REPL MODE [INTERACTIVE MODE] / BATCH MODE [AS A PROGRAM]
   
4. SPARK BATCH JOBS ARE SUBMITTED USING spark-submit COMMAND.
   PYTHON: <file>.py
   SCALA: <file>.scala --> COMPILES AS A jar FILE --> jar file is executable

    ex: spark-submit --master=yarn <file>.py

5. SPARK RUNS THE JOB WITHIN THE MEMORY AS "executors"

6. SPARK CAN DISTRIBUTE THE DATA INTO DIFFERENT EXECUTORS.

 SPARK V1
1. RDD BASED: RESILIENT DISTRIBUTED DATASETS
2. RDD IS CREATED USING SPARK CONTEXT OBJECT CALLED "sc"
3. RDD IS IMMUTABLE
4. RDD IS LAZY EXECUTION: NO RDD IS CREATED UNTIL ACTION IS GIVEN.
5. RDD IS SCHEMA LESS
6. RDD DO NOT USE OPTIMIZER:  LOWER LEVEL API

 SPARK V2
1. DATASET / DATAFRAME INTRODUCED AS HIGH LEVEL API
2. OPTIMIZER CALLED "CATALYST OPTIMIZER" --> TUNGSTEN EINGINE
3. SQL IS DEFAULT
4. NOW WE HAVE 'spark' AS SPARK SESSION OBJECT
    V1: sc.textFile(...)
    V2: spark.sparkContext.textFile(..) --> FOR CREATING RDD
        spark.read.... --> FOR CREATING DATAFRAME
5. SPARK SESSION IS AUTOMATICALLY CREATED IN REPL MODE, WE HAVE TO EXPLICTILY 
   CREATE IN BATCH MODE.
6. DATAFRAME IS LOGICAL WHICH REPRESENTS STRUCTURED DATA LIKE RDBMS TABLE

DATAFRAME
1. DATAFRAME IS A STRUCTURED LOGICAL OBJECT WHICH CONTAINS SCHEMA AND DATA [ROWS]

2. IT IS SIMILAR TO RDBMS TABLES.

3. DATAFRAME USES OPTIMIZER --> SQL ENGINE

4. DATAFRAME IS EAGER EXECUTION AT SCHEMA LEVEL, LAZY EXECUTION AT DATA LEVEL.

5. DATAFRAME CAN BE TRANSFORMED USING TRANSFORMATION OPERATIONS

6. DATAFRAME CAN BE CREATED BY READING THE FROM DIFFERENT SOURCES AND IN 
   DIFFERNT FORMATS.

CREATING DATAFRAME
1. FROM FILE
TEXT:  txt, csv, json, tsv
BINARY: parquet (default), avro, orc, sequence file

 spark.read.<fileformat>(<path>) or spark.read.format(<fileformat>).option

2. FROM TABLE
 RDBMS: jdbc ., HIVE METASTORE
   spark.read.table or spark.read.format(jdbc)

3. FROM MEMORY

4. FROM ANOTHER DATAFRAME [TRANSFORMATION]































































































    